#!/usr/bin/env python3
"""
æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆä¿®æ­£ç‰ˆï¼‰
å®Ÿéš›ã®SUUMOãƒ‡ãƒ¼ã‚¿ã®å–å¾—æ©Ÿèƒ½
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
import hashlib
import random
import logging
from datetime import datetime, date
from urllib.parse import urljoin, urlparse
import json
import os

class EnhancedRealEstateScraper:
    def __init__(self, db_path='realestate.db'):
        self.db_path = db_path
        self.scraped_count = 0
        self.session = requests.Session()
        
        # Headers for requests
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session.headers.update(self.headers)
        
        # Rate limiting
        self.rate_limits = {
            'suumo': {'min_delay': 5, 'max_delay': 10, 'max_pages': 10, 'max_items': 200},
        }
        
        # Logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Create necessary directories
        os.makedirs('cache', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
    
    def respectful_delay(self, site):
        """ã‚µã‚¤ãƒˆã«å¿œã˜ãŸé©åˆ‡ãªé…å»¶"""
        config = self.rate_limits.get(site, {'min_delay': 3, 'max_delay': 8})
        delay = random.uniform(config['min_delay'], config['max_delay'])
        self.logger.info(f"{site}: {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
        time.sleep(delay)
    
    def safe_request(self, url, max_retries=3):
        """å®‰å…¨ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                self.logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
                else:
                    self.logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ€çµ‚å¤±æ•—: {url}")
                    return None
    
    def enhanced_parse_suumo(self, html_content):
        """å®Ÿéš›ã®SUUMOå½¢å¼ã®è§£æ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        properties = []
        
        # property_unit-info ã‚¯ãƒ©ã‚¹ã‚’æ¢ã™ (å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹)
        property_items = soup.find_all('div', class_='property_unit-info')
        
        self.logger.info(f"ç‰©ä»¶è¦ç´ ã‚’ {len(property_items)} å€‹ç™ºè¦‹")
        
        for item in property_items:
            try:
                # ç‰©ä»¶åã®å–å¾—
                building_name = ""
                name_elem = item.find('dd')
                if name_elem:
                    building_name = name_elem.get_text(strip=True)
                
                # ä¾¡æ ¼ã®å–å¾—
                price = 0
                price_elem = item.find('span', class_='dottable-value')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price_match = re.search(r'(\d+)ä¸‡å††', price_text)
                    if price_match:
                        price = int(price_match.group(1)) * 10000
                
                # é–“å–ã‚Šã®å–å¾—
                layout = ""
                item_text = item.get_text()
                layout_patterns = [r'(\d+DK)', r'(\d+LDK)', r'(\d+K)', r'(\d+R)']
                for pattern in layout_patterns:
                    match = re.search(pattern, item_text)
                    if match:
                        layout = match.group(1)
                        break
                
                # é¢ç©ã®å–å¾—
                area = 0
                
                # æ–¹æ³•1: dt/ddè¦ç´ ã‹ã‚‰ã€Œå°‚æœ‰é¢ç©ã€ã‚’æ¢ã™
                area_elem = item.find('dt', text='å°‚æœ‰é¢ç©')
                if area_elem:
                    area_dd = area_elem.find_next('dd')
                    if area_dd:
                        area_text = area_dd.get_text(strip=True)
                        # ã€Œ25.61mÂ²ï¼ˆç™»è¨˜ï¼‰ã€ã‚„ã€Œ18.97m<sup>2</sup>ï¼ˆå£èŠ¯ï¼‰ã€å½¢å¼ã‹ã‚‰é¢ç©ã‚’æŠ½å‡º
                        area_patterns = [
                            r'(\d+(?:\.\d+)?)m.*?2',  # 25.61mÂ²ã‚„18.97m<sup>2</sup>ã«å¯¾å¿œ
                            r'(\d+(?:\.\d+)?)ã¡',
                            r'(\d+(?:\.\d+)?)å¹³ç±³'
                        ]
                        
                        for pattern in area_patterns:
                            area_match = re.search(pattern, area_text)
                            if area_match:
                                area = float(area_match.group(1))
                                break
                
                # æ–¹æ³•2: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰é¢ç©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                if area == 0:
                    area_patterns = [
                        r'(\d+(?:\.\d+)?)m.*?2',
                        r'(\d+(?:\.\d+)?)ã¡', 
                        r'(\d+(?:\.\d+)?)å¹³ç±³'
                    ]
                    
                    for pattern in area_patterns:
                        area_match = re.search(pattern, item_text)
                        if area_match:
                            area = float(area_match.group(1))
                            break
                
                # ä½æ‰€ã®å–å¾—
                address = "æ±äº¬éƒ½æ¸¯åŒº"
                address_elem = item.find('dt', text='æ‰€åœ¨åœ°')
                if address_elem:
                    address_dd = address_elem.find_next('dd')
                    if address_dd:
                        address = address_dd.get_text(strip=True)
                
                # ç¯‰å¹´æœˆãƒ»ç¯‰å¹´æ•°ã®å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                building_age = None
                construction_year = None
                construction_month = None
                construction_date = None
                
                # æ–¹æ³•1: dt/ddè¦ç´ ã‹ã‚‰ã€Œç¯‰å¹´æœˆã€ã‚’æ¢ã™
                building_age_elem = item.find('dt', text='ç¯‰å¹´æœˆ')
                if building_age_elem:
                    building_age_dd = building_age_elem.find_next('dd')
                    if building_age_dd:
                        building_age_text = building_age_dd.get_text(strip=True)
                        # ã€Œ1981å¹´12æœˆã€å½¢å¼ã‹ã‚‰ç¯‰å¹´æœˆã¨ç¯‰å¹´æ•°ã‚’æŠ½å‡º
                        year_month_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', building_age_text)
                        if year_month_match:
                            built_year = int(year_month_match.group(1))
                            built_month = int(year_month_match.group(2))
                            current_year = datetime.now().year
                            building_age = current_year - built_year
                            construction_year = built_year
                            construction_month = built_month
                            construction_date = f"{built_year}-{built_month:02d}-01"
                        else:
                            # å¹´ã®ã¿ã®å ´åˆ
                            year_match = re.search(r'(\d{4})å¹´', building_age_text)
                            if year_match:
                                built_year = int(year_match.group(1))
                                current_year = datetime.now().year
                                building_age = current_year - built_year
                                construction_year = built_year
                
                # æ–¹æ³•2: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¯‰å¹´æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                if building_age is None:
                    age_patterns = [
                        r'ç¯‰(\d+)å¹´',
                        r'ç¯‰å¹´æ•°(\d+)å¹´',
                        r'ç¯‰(\d+)',
                        r'(\d{4})å¹´(\d{1,2})æœˆç¯‰'
                    ]
                    
                    for pattern in age_patterns:
                        age_match = re.search(pattern, item_text)
                        if age_match:
                            if pattern.startswith(r'(\d{4})'):
                                # å¹´æœˆå½¢å¼ã®å ´åˆã¯è¨ˆç®—
                                built_year = int(age_match.group(1))
                                current_year = datetime.now().year
                                building_age = current_year - built_year
                            else:
                                # ç›´æ¥ã®ç¯‰å¹´æ•°
                                building_age = int(age_match.group(1))
                            break
                
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã¨ç‰©ä»¶IDã‚’å–å¾—
                detail_url = ""
                source_property_id = ""
                # è¦ªè¦ç´ ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                parent_link = item.find_parent().find('a', href=True)
                if parent_link:
                    detail_url = parent_link['href']
                    if not detail_url.startswith('http'):
                        detail_url = f"https://suumo.jp{detail_url}"
                    
                    # URLã‹ã‚‰ç‰©ä»¶IDã‚’æŠ½å‡º
                    property_id_match = re.search(r'/nc_(\d+)/', detail_url)
                    if property_id_match:
                        source_property_id = f"nc_{property_id_match.group(1)}"
                
                # æœ€ä½é™ã®æƒ…å ±ãŒã‚ã‚Œã°ä¿å­˜
                if price > 0:
                    property_data = {
                        'address': address,
                        'building_name': building_name or "ç‰©ä»¶åä¸æ˜",
                        'room_layout': layout or "é–“å–ã‚Šä¸æ˜",
                        'floor_area': area or 0,
                        'floor': None,
                        'building_age': building_age,
                        'construction_year': construction_year,
                        'construction_month': construction_month,
                        'construction_date': construction_date,
                        'structure': None,
                        'current_price': price,
                        'management_fee': 0,
                        'transport_info': "",
                        'source_site': 'suumo',
                        'source_url': detail_url,
                        'source_property_id': source_property_id,
                        'agent_company': 'SUUMOæ²è¼‰',
                        'first_listed_at': date.today().isoformat()
                    }
                    
                    properties.append(property_data)
                    
            except Exception as e:
                self.logger.warning(f"ç‰©ä»¶æƒ…å ±è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return properties
    
    def enhanced_scrape_suumo(self, area_code='13103'):
        """æ”¹è‰¯ç‰ˆSUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.logger.info("ğŸ  æ”¹è‰¯ç‰ˆSUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        
        base_url = 'https://suumo.jp/ms/chuko/tokyo/sc_minato/'
        
        count = 0
        max_pages = self.rate_limits['suumo']['max_pages']
        max_items = self.rate_limits['suumo']['max_items']
        
        for page in range(1, max_pages + 1):
            if count >= max_items:
                self.logger.info(f"æœ€å¤§å–å¾—ä»¶æ•° ({max_items}) ã«é”ã—ã¾ã—ãŸ")
                break
            
            if page == 1:
                url = base_url
            else:
                url = f"{base_url}?page={page}"
            
            try:
                self.logger.info(f"ğŸ“„ SUUMOãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å‡¦ç†ä¸­...")
                
                if page > 1:
                    self.respectful_delay('suumo')
                
                html_content = self.safe_request(url)
                if not html_content:
                    self.logger.error(f"ãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    continue
                
                properties = self.enhanced_parse_suumo(html_content)
                
                if not properties:
                    self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã«ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    break
                
                page_saved = 0
                for prop in properties:
                    if count >= max_items:
                        break
                    
                    if self.save_property_enhanced(prop):
                        page_saved += 1
                        count += 1
                        self.scraped_count += 1
                
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} å®Œäº†: {page_saved} ä»¶ä¿å­˜")
                
            except Exception as e:
                self.logger.error(f"SUUMOãƒšãƒ¼ã‚¸ {page} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        self.logger.info(f"âœ… SUUMOå®Œäº†: {count} ä»¶ã®ç‰©ä»¶ã‚’å–å¾—")
        return count
    
    def calculate_property_hash(self, property_data):
        """ç‰©ä»¶ã®ä¸€æ„æ€§ã‚’åˆ¤å®šã™ã‚‹ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        hash_string = f"{property_data['address']}_{property_data['room_layout']}_{property_data['floor_area']}_{property_data['building_age']}"
        return hashlib.md5(hash_string.encode()).hexdigest()
    
    def save_property_enhanced(self, property_data):
        """æ”¹è‰¯ç‰ˆç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
            prop_hash = self.calculate_property_hash(property_data)
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            cursor.execute('SELECT id FROM properties WHERE master_property_hash = ?', (prop_hash,))
            existing = cursor.fetchone()
            
            if existing:
                self.logger.debug(f"é‡è¤‡ç‰©ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—: {property_data['building_name']}")
                return False
            
            # ã‚¨ãƒªã‚¢ã®å–å¾—ã¾ãŸã¯ä½œæˆ
            cursor.execute('SELECT id FROM areas WHERE ward_name = ?', ('æ¸¯åŒº',))
            area_result = cursor.fetchone()
            
            if area_result:
                area_id = area_result[0]
            else:
                cursor.execute('''
                    INSERT INTO areas (prefecture_name, ward_name) 
                    VALUES (?, ?)
                ''', ('æ±äº¬éƒ½', 'æ¸¯åŒº'))
                area_id = cursor.lastrowid
            
            # ç‰©ä»¶ã‚’ä¿å­˜
            cursor.execute('''
                INSERT INTO properties (
                    area_id, address, room_layout, floor_area, current_price,
                    first_listed_at, building_name, building_age, 
                    construction_year, construction_month, construction_date,
                    master_property_hash, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            ''', (
                area_id, property_data['address'], property_data['room_layout'],
                property_data['floor_area'], property_data['current_price'],
                property_data['first_listed_at'], property_data['building_name'],
                property_data['building_age'], property_data['construction_year'],
                property_data['construction_month'], property_data['construction_date'],
                prop_hash
            ))
            
            property_id = cursor.lastrowid
            
            # ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
            cursor.execute('''
                INSERT INTO property_listings (
                    property_id, source_site, agent_company, listed_price,
                    source_url, source_property_id, is_active, scraped_at
                ) VALUES (?, ?, ?, ?, ?, ?, 1, CURRENT_TIMESTAMP)
            ''', (
                property_id, property_data['source_site'], property_data['agent_company'],
                property_data['current_price'], property_data['source_url'],
                property_data['source_property_id']
            ))
            
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç‰©ä»¶ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_enhanced_scraping(self):
        """æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        self.logger.info("=" * 50)
        
        total_count = 0
        
        try:
            # SUUMO
            self.logger.info("æ”¹è‰¯ç‰ˆSUUMOå‡¦ç†é–‹å§‹...")
            suumo_count = self.enhanced_scrape_suumo()
            total_count += suumo_count
            
            # æ¬¡ã®ã‚µã‚¤ãƒˆã¸ã®å¾…æ©Ÿæ™‚é–“
            if total_count > 0:
                delay = random.uniform(15, 25)
                self.logger.info(f"æ¬¡ã®ã‚µã‚¤ãƒˆã¾ã§ {delay:.1f}ç§’å¾…æ©Ÿ...")
                time.sleep(delay)
            
        except Exception as e:
            self.logger.error(f"SUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.logger.info("=" * 50)
        self.logger.info(f"ğŸ‰ æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: åˆè¨ˆ {total_count} ä»¶")
        
        return total_count
    
    def print_stats(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã‚’è¡¨ç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM properties')
        total_properties = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM property_listings')
        total_listings = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT source_site, COUNT(*) 
            FROM property_listings 
            GROUP BY source_site
        ''')
        site_stats = cursor.fetchall()
        
        print("\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ:")
        print(f"ç·ç‰©ä»¶æ•°: {total_properties}")
        print(f"ç·ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°æ•°: {total_listings}")
        print("ã‚µã‚¤ãƒˆåˆ¥ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°:")
        for site, count in site_stats:
            print(f"  - {site}: {count} ä»¶")
        
        conn.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = EnhancedRealEstateScraper()
    
    print("ğŸš€ æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³")
    print("=" * 40)
    
    # çµ±è¨ˆè¡¨ç¤º
    scraper.print_stats()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraped_count = scraper.run_enhanced_scraping()
    
    print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {scraped_count} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

if __name__ == "__main__":
    main()