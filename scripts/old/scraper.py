#!/usr/bin/env python3
"""
‰∏çÂãïÁî£„Çµ„Ç§„Éà„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞Ê©üËÉΩ
SUUMO„ÄÅ„Ç¢„ÉÉ„Éà„Éõ„Éº„É†„ÄÅ„Éõ„Éº„É†„Ç∫„Åã„ÇâÁâ©‰ª∂ÊÉÖÂ†±„ÇíÂèéÈõÜ
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import json
import re
import hashlib
from datetime import datetime, date
from urllib.parse import urljoin, urlparse
import sys
import random
import logging
from urllib.robotparser import RobotFileParser

class RealEstateScraper:
    def __init__(self, db_path='realestate.db'):
        self.db_path = db_path
        self.session = requests.Session()
        
        # Ë¶èÁ¥ÑÈÅµÂÆà„ÅÆ„Åü„ÇÅ„ÅÆË®≠ÂÆö
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # „É¨„Éº„ÉàÂà∂ÈôêË®≠ÂÆö
        self.rate_limits = {
            'suumo': {'min_delay': 3, 'max_delay': 6, 'max_pages': 5},
            'athome': {'min_delay': 4, 'max_delay': 8, 'max_pages': 3},
            'homes': {'min_delay': 5, 'max_delay': 10, 'max_pages': 3}
        }
        
        self.scraped_count = 0
        self.robots_cache = {}
        
        # „É≠„Ç∞Ë®≠ÂÆö
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
    def get_db_connection(self):
        """„Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂ö„ÇíÂèñÂæó"""
        conn = sqlite3.connect(self.db_path)
        return conn
    
    def check_robots_txt(self, site_url, user_agent='*'):
        """robots.txt„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Ç¢„ÇØ„Çª„ÇπÂèØËÉΩ„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç"""
        try:
            if site_url not in self.robots_cache:
                robots_url = urljoin(site_url, '/robots.txt')
                rp = RobotFileParser()
                rp.set_url(robots_url)
                rp.read()
                self.robots_cache[site_url] = rp
            
            return self.robots_cache[site_url]
        except:
            # robots.txt„ÅåÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÂà∂Èôê„Å™„Åó„Å®„Åó„Å¶Êâ±„ÅÜ
            return None
    
    def can_fetch(self, site_url, path, user_agent='*'):
        """ÊåáÂÆö„ÅÆ„Éë„Çπ„Å´„Ç¢„ÇØ„Çª„ÇπÂèØËÉΩ„Åã„Å©„ÅÜ„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ"""
        rp = self.check_robots_txt(site_url, user_agent)
        if rp:
            return rp.can_fetch(user_agent, path)
        return True
    
    def respectful_delay(self, site_name):
        """„Çµ„Ç§„Éà„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™ÈÅÖÂª∂„ÇíÂÆüË°å"""
        if site_name in self.rate_limits:
            min_delay = self.rate_limits[site_name]['min_delay']
            max_delay = self.rate_limits[site_name]['max_delay']
            delay = random.uniform(min_delay, max_delay)
            self.logger.info(f"{site_name}: {delay:.1f}ÁßíÂæÖÊ©ü‰∏≠...")
            time.sleep(delay)
        else:
            time.sleep(random.uniform(2, 5))  # „Éá„Éï„Ç©„É´„ÉàÈÅÖÂª∂
    
    def save_property(self, property_data):
        """Áâ©‰ª∂„Éá„Éº„Çø„Çí„Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # Áâ©‰ª∂„ÅÆ„Éè„ÉÉ„Ç∑„É•„ÇíÁîüÊàêÔºàÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØÁî®Ôºâ
            hash_data = f"{property_data['address']}{property_data['room_layout']}{property_data['floor_area']}"
            property_hash = hashlib.md5(hash_data.encode()).hexdigest()
            
            # Êó¢Â≠òÁâ©‰ª∂„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            cursor.execute('SELECT id FROM properties WHERE master_property_hash = ?', (property_hash,))
            existing = cursor.fetchone()
            
            if existing:
                property_id = existing[0]
                # ‰æ°Ê†º„ÇíÊõ¥Êñ∞
                cursor.execute('''
                    UPDATE properties 
                    SET current_price = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (property_data['current_price'], property_id))
            else:
                # Êñ∞Ë¶èÁâ©‰ª∂„ÇíÊåøÂÖ•
                cursor.execute('''
                    INSERT INTO properties 
                    (area_id, address, building_name, room_layout, floor_area, building_age, 
                     current_price, first_listed_at, master_property_hash, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                ''', (
                    property_data['area_id'],
                    property_data['address'],
                    property_data.get('building_name', ''),
                    property_data['room_layout'],
                    property_data['floor_area'],
                    property_data.get('building_age'),
                    property_data['current_price'],
                    property_data.get('first_listed_at', date.today().isoformat()),
                    property_hash
                ))
                property_id = cursor.lastrowid
            
            # „É™„Çπ„ÉÜ„Ç£„É≥„Ç∞ÊÉÖÂ†±„Çí‰øùÂ≠ò
            listing_id = hashlib.md5(f"{property_data['source_site']}{property_data['source_url']}".encode()).hexdigest()
            cursor.execute('''
                INSERT OR REPLACE INTO property_listings
                (property_id, source_site, source_url, listing_id, agent_company, 
                 listed_price, is_active, scraped_at, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            ''', (
                property_id,
                property_data['source_site'],
                property_data['source_url'],
                listing_id,
                property_data.get('agent_company', ''),
                property_data['current_price']
            ))
            
            conn.commit()
            return property_id
            
        except Exception as e:
            print(f"„Éá„Éº„Çø„Éô„Éº„Çπ‰øùÂ≠ò„Ç®„É©„Éº: {e}")
            conn.rollback()
            return None
        finally:
            conn.close()
    
    def scrape_suumo(self, area_code='13103'):
        """SUUMO„Åã„ÇâÁâ©‰ª∂ÊÉÖÂ†±„ÇíÂèñÂæóÔºàË¶èÁ¥ÑÈÅµÂÆàÁâàÔºâ"""
        self.logger.info("üè† SUUMO„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã...")
        
        base_url = 'https://suumo.jp/jj/bukken/ichiran/JJ010FV001/'
        site_url = 'https://suumo.jp'
        
        # robots.txt„ÉÅ„Çß„ÉÉ„ÇØ
        if not self.can_fetch(site_url, '/jj/bukken/ichiran/JJ010FV001/'):
            self.logger.warning("‚ö†Ô∏è  robots.txt„Å´„Çà„ÇäSUUMO„Å∏„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÅåÂà∂Èôê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô")
            return 0
        
        params = {
            'ar': '030',      # Èñ¢Êù±
            'bs': '040',      # ‰∏≠Âè§„Éû„É≥„Ç∑„Éß„É≥
            'ta': '13',       # Êù±‰∫¨ÈÉΩ
            'sc': area_code,  # Ê∏ØÂå∫
            'pn': 1
        }
        
        count = 0
        max_pages = self.rate_limits['suumo']['max_pages']
        
        for page in range(1, max_pages + 1):
            params['pn'] = page
            
            try:
                self.logger.info(f"üìÑ SUUMO„Éö„Éº„Ç∏ {page}/{max_pages} „ÇíÂá¶ÁêÜ‰∏≠...")
                
                # ÈÅ©Âàá„Å™ÈÅÖÂª∂„ÇíÂÆüË°å
                if page > 1:
                    self.respectful_delay('suumo')
                
                response = self.session.get(base_url, params=params, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Áâ©‰ª∂„É™„Çπ„Éà„ÇíÂèñÂæó
                property_items = soup.find_all('div', class_='cassetteitem')
                
                if not property_items:
                    self.logger.info(f"„Éö„Éº„Ç∏ {page} „Å´Áâ©‰ª∂„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                    break
                
                for item in property_items:
                    page_count = self.parse_suumo_item(item)
                    count += page_count
                
                self.logger.info(f"„Éö„Éº„Ç∏ {page} ÂÆå‰∫Ü: {len(property_items)} ‰ª∂Âá¶ÁêÜ")
                
            except requests.RequestException as e:
                self.logger.error(f"SUUMO„Éö„Éº„Ç∏ {page} „ÅÆÂèñÂæó„Ç®„É©„Éº: {e}")
                break
            except Exception as e:
                self.logger.error(f"SUUMO„Éö„Éº„Ç∏ {page} „ÅÆËß£Êûê„Ç®„É©„Éº: {e}")
                continue
        
        self.logger.info(f"‚úÖ SUUMOÂÆå‰∫Ü: {count} ‰ª∂„ÅÆÁâ©‰ª∂„ÇíÂèñÂæó")
        return count
    
    def parse_suumo_item(self, item):
        """SUUMOÁâ©‰ª∂„Ç¢„Ç§„ÉÜ„É†„ÇíËß£Êûê"""
        count = 0
        
        try:
            # Âü∫Êú¨ÊÉÖÂ†±
            title_elem = item.find('div', class_='cassetteitem_content-title')
            if not title_elem:
                return 0
            
            building_name = title_elem.get_text(strip=True)
            
            # ‰ΩèÊâÄÊÉÖÂ†±
            address_elem = item.find('li', class_='cassetteitem_detail-col1')
            if not address_elem:
                return 0
            
            address = address_elem.get_text(strip=True)
            
            # ÂêÑÈÉ®Â±ã„ÅÆÊÉÖÂ†±
            room_items = item.find_all('tbody')
            for tbody in room_items:
                rows = tbody.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 8:
                        # ‰æ°Ê†º
                        price_text = cols[2].get_text(strip=True)
                        price = self.parse_price(price_text)
                        if price == 0:
                            continue
                        
                        # ÈñìÂèñ„Çä
                        layout_text = cols[3].get_text(strip=True)
                        
                        # Â∞ÇÊúâÈù¢Á©ç
                        area_text = cols[4].get_text(strip=True)
                        floor_area = self.parse_area(area_text)
                        if floor_area == 0:
                            continue
                        
                        # ÁØâÂπ¥Êï∞
                        age_text = cols[5].get_text(strip=True)
                        building_age = self.parse_building_age(age_text)
                        
                        # Ë©≥Á¥∞„É™„É≥„ÇØ
                        link_elem = cols[8].find('a')
                        detail_url = ''
                        if link_elem and link_elem.get('href'):
                            detail_url = urljoin('https://suumo.jp', link_elem['href'])
                        
                        # Áâ©‰ª∂„Éá„Éº„Çø„ÇíÊßãÁØâ
                        property_data = {
                            'area_id': 1,  # Ê∏ØÂå∫
                            'address': address,
                            'building_name': building_name,
                            'room_layout': layout_text,
                            'floor_area': floor_area,
                            'building_age': building_age,
                            'current_price': price,
                            'source_site': 'suumo',
                            'source_url': detail_url,
                            'agent_company': 'SUUMOÊé≤Ëºâ',
                            'first_listed_at': date.today().isoformat()
                        }
                        
                        # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
                        if self.save_property(property_data):
                            count += 1
                            self.scraped_count += 1
                            
                            if count <= 3:  # ÊúÄÂàù„ÅÆ3‰ª∂„Å†„ÅëË©≥Á¥∞Ë°®Á§∫
                                print(f"  üìç {address} {layout_text} {floor_area}„é° {price:,}ÂÜÜ")
        
        except Exception as e:
            print(f"SUUMOÁâ©‰ª∂Ëß£Êûê„Ç®„É©„Éº: {e}")
        
        return count
    
    def scrape_athome(self, area_code='tokyo/minato-city'):
        """„Ç¢„ÉÉ„Éà„Éõ„Éº„É†„Åã„ÇâÁâ©‰ª∂ÊÉÖÂ†±„ÇíÂèñÂæóÔºàË¶èÁ¥ÑÈÅµÂÆàÁâàÔºâ"""
        self.logger.info("üè† „Ç¢„ÉÉ„Éà„Éõ„Éº„É†„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã...")
        
        base_url = f'https://www.athome.co.jp/kodate/chuko/{area_code}/list/'
        site_url = 'https://www.athome.co.jp'
        
        # robots.txt„ÉÅ„Çß„ÉÉ„ÇØ
        if not self.can_fetch(site_url, f'/kodate/chuko/{area_code}/list/'):
            self.logger.warning("‚ö†Ô∏è  robots.txt„Å´„Çà„Çä„Ç¢„ÉÉ„Éà„Éõ„Éº„É†„Å∏„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÅåÂà∂Èôê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô")
            return 0
        
        try:
            self.logger.info("üìÑ „Ç¢„ÉÉ„Éà„Éõ„Éº„É†Áâ©‰ª∂„É™„Çπ„Éà„ÇíÂèñÂæó‰∏≠...")
            
            # ÈÅ©Âàá„Å™ÈÅÖÂª∂„ÇíÂÆüË°å
            self.respectful_delay('athome')
            
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Áâ©‰ª∂„É™„Çπ„Éà„ÇíÂèñÂæó
            property_items = soup.find_all('div', class_='property-unit')
            
            if not property_items:
                self.logger.info("„Ç¢„ÉÉ„Éà„Éõ„Éº„É†„ÅßÁâ©‰ª∂„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü")
                return 0
            
            count = 0
            max_items = min(len(property_items), 10)  # ÊúÄÂ§ß10‰ª∂„Åæ„Åß
            
            for i, item in enumerate(property_items[:max_items]):
                if i > 0:
                    # ÂêÑÁâ©‰ª∂Èñì„Åß„ÇÇÈÅ©Âàá„Å™ÈÅÖÂª∂
                    time.sleep(random.uniform(1, 2))
                
                if self.parse_athome_item(item):
                    count += 1
                    self.scraped_count += 1
            
            self.logger.info(f"‚úÖ „Ç¢„ÉÉ„Éà„Éõ„Éº„É†ÂÆå‰∫Ü: {count} ‰ª∂„ÅÆÁâ©‰ª∂„ÇíÂèñÂæó")
            return count
            
        except requests.RequestException as e:
            self.logger.error(f"„Ç¢„ÉÉ„Éà„Éõ„Éº„É†ÂèñÂæó„Ç®„É©„Éº: {e}")
            return 0
        except Exception as e:
            self.logger.error(f"„Ç¢„ÉÉ„Éà„Éõ„Éº„É†Ëß£Êûê„Ç®„É©„Éº: {e}")
            return 0
    
    def parse_athome_item(self, item):
        """„Ç¢„ÉÉ„Éà„Éõ„Éº„É†Áâ©‰ª∂„Ç¢„Ç§„ÉÜ„É†„ÇíËß£Êûê"""
        try:
            # ‰æ°Ê†º
            price_elem = item.find('span', class_='price')
            if not price_elem:
                return False
            
            price_text = price_elem.get_text(strip=True)
            price = self.parse_price(price_text)
            if price == 0:
                return False
            
            # ‰ΩèÊâÄ
            address_elem = item.find('div', class_='address')
            if not address_elem:
                return False
            
            address = address_elem.get_text(strip=True)
            
            # ÈñìÂèñ„Çä„ÉªÈù¢Á©ç
            details = item.find('div', class_='property-detail')
            if not details:
                return False
            
            detail_text = details.get_text(strip=True)
            
            # ÈñìÂèñ„Çä„ÇíÊäΩÂá∫
            layout_match = re.search(r'(\d+[SLDK]+)', detail_text)
            room_layout = layout_match.group(1) if layout_match else '‰∏çÊòé'
            
            # Èù¢Á©ç„ÇíÊäΩÂá∫
            area_match = re.search(r'(\d+\.?\d*)„é°', detail_text)
            floor_area = float(area_match.group(1)) if area_match else 0
            
            if floor_area == 0:
                return False
            
            # ÁØâÂπ¥Êï∞„ÇíÊäΩÂá∫
            age_match = re.search(r'ÁØâ(\d+)Âπ¥', detail_text)
            building_age = int(age_match.group(1)) if age_match else None
            
            # Ë©≥Á¥∞„É™„É≥„ÇØ
            link_elem = item.find('a')
            detail_url = ''
            if link_elem and link_elem.get('href'):
                detail_url = urljoin('https://www.athome.co.jp', link_elem['href'])
            
            # Áâ©‰ª∂„Éá„Éº„Çø„ÇíÊßãÁØâ
            property_data = {
                'area_id': 1,  # Ê∏ØÂå∫
                'address': address,
                'building_name': '',
                'room_layout': room_layout,
                'floor_area': floor_area,
                'building_age': building_age,
                'current_price': price,
                'source_site': 'athome',
                'source_url': detail_url,
                'agent_company': '„Ç¢„ÉÉ„Éà„Éõ„Éº„É†Êé≤Ëºâ',
                'first_listed_at': date.today().isoformat()
            }
            
            # „Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò
            if self.save_property(property_data):
                print(f"  üìç {address} {room_layout} {floor_area}„é° {price:,}ÂÜÜ")
                return True
            
        except Exception as e:
            print(f"„Ç¢„ÉÉ„Éà„Éõ„Éº„É†Áâ©‰ª∂Ëß£Êûê„Ç®„É©„Éº: {e}")
        
        return False
    
    def parse_price(self, price_text):
        """‰æ°Ê†ºÊñáÂ≠óÂàó„ÇíÊï∞ÂÄ§„Å´Â§âÊèõ"""
        try:
            # Êï∞Â≠ó„Å®Âçò‰Ωç„ÇíÊäΩÂá∫
            numbers = re.findall(r'[\d,]+', price_text)
            if not numbers:
                return 0
            
            price_str = numbers[0].replace(',', '')
            price = int(price_str)
            
            # Âçò‰Ωç„ÇíÁ¢∫Ë™ç
            if 'ÂÑÑ' in price_text:
                price *= 100000000
            elif '‰∏á' in price_text:
                price *= 10000
            
            return price
            
        except (ValueError, IndexError):
            return 0
    
    def parse_area(self, area_text):
        """Èù¢Á©çÊñáÂ≠óÂàó„ÇíÊï∞ÂÄ§„Å´Â§âÊèõ"""
        try:
            match = re.search(r'(\d+\.?\d*)', area_text)
            if match:
                return float(match.group(1))
            return 0
        except ValueError:
            return 0
    
    def parse_building_age(self, age_text):
        """ÁØâÂπ¥Êï∞ÊñáÂ≠óÂàó„ÇíÊï∞ÂÄ§„Å´Â§âÊèõ"""
        try:
            match = re.search(r'(\d+)', age_text)
            if match:
                return int(match.group(1))
            return None
        except ValueError:
            return None
    
    def scrape_all(self):
        """ÂÖ®„Çµ„Ç§„Éà„Åã„Çâ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆüË°åÔºàË¶èÁ¥ÑÈÅµÂÆàÁâàÔºâ"""
        self.logger.info("üöÄ ‰∏çÂãïÁî£„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã")
        self.logger.info("=" * 50)
        self.logger.info("‚ö†Ô∏è  ÂêÑ„Çµ„Ç§„Éà„ÅÆÂà©Áî®Ë¶èÁ¥Ñ„Å®robots.txt„ÇíÈÅµÂÆà„Åó„Å¶„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„ÇíÂÆüË°å„Åó„Åæ„Åô")
        self.logger.info("üìã ÂèñÂæóÂà∂Èôê: SUUMOÊúÄÂ§ß5„Éö„Éº„Ç∏„ÄÅ„Ç¢„ÉÉ„Éà„Éõ„Éº„É†ÊúÄÂ§ß10‰ª∂")
        
        total_count = 0
        
        # SUUMO
        try:
            self.logger.info("1/2 SUUMOÂá¶ÁêÜÈñãÂßã...")
            suumo_count = self.scrape_suumo()
            total_count += suumo_count
            
            # „Çµ„Ç§„ÉàÈñì„ÅÆÂçÅÂàÜ„Å™ÈñìÈöî
            inter_site_delay = random.uniform(10, 15)
            self.logger.info(f"Ê¨°„ÅÆ„Çµ„Ç§„Éà„Åæ„Åß {inter_site_delay:.1f}ÁßíÂæÖÊ©ü...")
            time.sleep(inter_site_delay)
            
        except Exception as e:
            self.logger.error(f"SUUMO„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Ç®„É©„Éº: {e}")
        
        # „Ç¢„ÉÉ„Éà„Éõ„Éº„É†
        try:
            self.logger.info("2/2 „Ç¢„ÉÉ„Éà„Éõ„Éº„É†Âá¶ÁêÜÈñãÂßã...")
            athome_count = self.scrape_athome()
            total_count += athome_count
            
        except Exception as e:
            self.logger.error(f"„Ç¢„ÉÉ„Éà„Éõ„Éº„É†„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Ç®„É©„Éº: {e}")
        
        self.logger.info("=" * 50)
        self.logger.info(f"üéâ „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÂÆå‰∫Ü: ÂêàË®à {total_count} ‰ª∂„ÅÆÁâ©‰ª∂„ÇíÂèñÂæó")
        self.logger.info("üìä Ë¶èÁ¥ÑÈÅµÂÆà„ÅÆ„Åü„ÇÅ„ÄÅÂèñÂæó‰ª∂Êï∞„ÇíÂà∂Èôê„Åó„Å¶„ÅÑ„Åæ„Åô")
        
        return total_count

def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    if len(sys.argv) > 1 and sys.argv[1] == '--area':
        area = sys.argv[2] if len(sys.argv) > 2 else 'minato'
        print(f"ÂØæË±°„Ç®„É™„Ç¢: {area}")
    
    scraper = RealEstateScraper()
    
    try:
        total_count = scraper.scrape_all()
        print(f"\nüìä „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÁµêÊûú: {total_count} ‰ª∂")
        
        # ÁµêÊûúÁ¢∫Ë™ç
        conn = scraper.get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM properties')
        total_properties = cursor.fetchone()[0]
        print(f"üíæ „Éá„Éº„Çø„Éô„Éº„ÇπÂÜÖÁ∑èÁâ©‰ª∂Êï∞: {total_properties} ‰ª∂")
        conn.close()
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü")
    except Exception as e:
        print(f"‚ùå „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Ç®„É©„Éº: {e}")

if __name__ == '__main__':
    main()