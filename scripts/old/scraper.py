#!/usr/bin/env python3
"""
ä¸å‹•ç”£ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
SUUMOã€ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã€ãƒ›ãƒ¼ãƒ ã‚ºã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’åé›†
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import json
import re
import hashlib
from datetime import datetime, date
from urllib.parse import urljoin, urlparse
import sys
import random
import logging
from urllib.robotparser import RobotFileParser

class RealEstateScraper:
    def __init__(self, db_path='realestate.db'):
        self.db_path = db_path
        self.session = requests.Session()
        
        # è¦ç´„éµå®ˆã®ãŸã‚ã®è¨­å®š
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š
        self.rate_limits = {
            'suumo': {'min_delay': 3, 'max_delay': 6, 'max_pages': 5},
            'athome': {'min_delay': 4, 'max_delay': 8, 'max_pages': 3},
            'homes': {'min_delay': 5, 'max_delay': 10, 'max_pages': 3}
        }
        
        self.scraped_count = 0
        self.robots_cache = {}
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
    def get_db_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        return conn
    
    def check_robots_txt(self, site_url, user_agent='*'):
        """robots.txtã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        try:
            if site_url not in self.robots_cache:
                robots_url = urljoin(site_url, '/robots.txt')
                rp = RobotFileParser()
                rp.set_url(robots_url)
                rp.read()
                self.robots_cache[site_url] = rp
            
            return self.robots_cache[site_url]
        except:
            # robots.txtãŒå–å¾—ã§ããªã„å ´åˆã¯åˆ¶é™ãªã—ã¨ã—ã¦æ‰±ã†
            return None
    
    def can_fetch(self, site_url, path, user_agent='*'):
        """æŒ‡å®šã®ãƒ‘ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        rp = self.check_robots_txt(site_url, user_agent)
        if rp:
            return rp.can_fetch(user_agent, path)
        return True
    
    def respectful_delay(self, site_name):
        """ã‚µã‚¤ãƒˆã«å¿œã˜ãŸé©åˆ‡ãªé…å»¶ã‚’å®Ÿè¡Œ"""
        if site_name in self.rate_limits:
            min_delay = self.rate_limits[site_name]['min_delay']
            max_delay = self.rate_limits[site_name]['max_delay']
            delay = random.uniform(min_delay, max_delay)
            self.logger.info(f"{site_name}: {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(delay)
        else:
            time.sleep(random.uniform(2, 5))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé…å»¶
    
    def save_property(self, property_data):
        """ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # ç‰©ä»¶ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
            hash_data = f"{property_data['address']}{property_data['room_layout']}{property_data['floor_area']}"
            property_hash = hashlib.md5(hash_data.encode()).hexdigest()
            
            # æ—¢å­˜ç‰©ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            cursor.execute('SELECT id FROM properties WHERE master_property_hash = ?', (property_hash,))
            existing = cursor.fetchone()
            
            if existing:
                property_id = existing[0]
                # ä¾¡æ ¼ã‚’æ›´æ–°
                cursor.execute('''
                    UPDATE properties 
                    SET current_price = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (property_data['current_price'], property_id))
            else:
                # æ–°è¦ç‰©ä»¶ã‚’æŒ¿å…¥
                cursor.execute('''
                    INSERT INTO properties 
                    (area_id, address, building_name, room_layout, floor_area, building_age, 
                     current_price, first_listed_at, master_property_hash, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                ''', (
                    property_data['area_id'],
                    property_data['address'],
                    property_data.get('building_name', ''),
                    property_data['room_layout'],
                    property_data['floor_area'],
                    property_data.get('building_age'),
                    property_data['current_price'],
                    property_data.get('first_listed_at', date.today().isoformat()),
                    property_hash
                ))
                property_id = cursor.lastrowid
            
            # ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
            listing_id = hashlib.md5(f"{property_data['source_site']}{property_data['source_url']}".encode()).hexdigest()
            cursor.execute('''
                INSERT OR REPLACE INTO property_listings
                (property_id, source_site, source_url, listing_id, agent_company, 
                 listed_price, is_active, scraped_at, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            ''', (
                property_id,
                property_data['source_site'],
                property_data['source_url'],
                listing_id,
                property_data.get('agent_company', ''),
                property_data['current_price']
            ))
            
            conn.commit()
            return property_id
            
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            conn.rollback()
            return None
        finally:
            conn.close()
    
    def scrape_suumo(self, area_code='13103'):
        """SUUMOã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ï¼ˆè¦ç´„éµå®ˆç‰ˆï¼‰"""
        self.logger.info("ğŸ  SUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        
        base_url = 'https://suumo.jp/jj/bukken/ichiran/JJ010FV001/'
        site_url = 'https://suumo.jp'
        
        # robots.txtãƒã‚§ãƒƒã‚¯
        if not self.can_fetch(site_url, '/jj/bukken/ichiran/JJ010FV001/'):
            self.logger.warning("âš ï¸  robots.txtã«ã‚ˆã‚ŠSUUMOã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™")
            return 0
        
        params = {
            'ar': '030',      # é–¢æ±
            'bs': '040',      # ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³
            'ta': '13',       # æ±äº¬éƒ½
            'sc': area_code,  # æ¸¯åŒº
            'pn': 1
        }
        
        count = 0
        max_pages = self.rate_limits['suumo']['max_pages']
        
        for page in range(1, max_pages + 1):
            params['pn'] = page
            
            try:
                self.logger.info(f"ğŸ“„ SUUMOãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å‡¦ç†ä¸­...")
                
                # é©åˆ‡ãªé…å»¶ã‚’å®Ÿè¡Œ
                if page > 1:
                    self.respectful_delay('suumo')
                
                response = self.session.get(base_url, params=params, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’å–å¾—
                property_items = soup.find_all('div', class_='cassetteitem')
                
                if not property_items:
                    self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã«ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    break
                
                for item in property_items:
                    page_count = self.parse_suumo_item(item)
                    count += page_count
                
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} å®Œäº†: {len(property_items)} ä»¶å‡¦ç†")
                
            except requests.RequestException as e:
                self.logger.error(f"SUUMOãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                break
            except Exception as e:
                self.logger.error(f"SUUMOãƒšãƒ¼ã‚¸ {page} ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        self.logger.info(f"âœ… SUUMOå®Œäº†: {count} ä»¶ã®ç‰©ä»¶ã‚’å–å¾—")
        return count
    
    def parse_suumo_item(self, item):
        """SUUMOç‰©ä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚’è§£æ"""
        count = 0
        
        try:
            # åŸºæœ¬æƒ…å ±
            title_elem = item.find('div', class_='cassetteitem_content-title')
            if not title_elem:
                return 0
            
            building_name = title_elem.get_text(strip=True)
            
            # ä½æ‰€æƒ…å ±
            address_elem = item.find('li', class_='cassetteitem_detail-col1')
            if not address_elem:
                return 0
            
            address = address_elem.get_text(strip=True)
            
            # å„éƒ¨å±‹ã®æƒ…å ±
            room_items = item.find_all('tbody')
            for tbody in room_items:
                rows = tbody.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 8:
                        # ä¾¡æ ¼
                        price_text = cols[2].get_text(strip=True)
                        price = self.parse_price(price_text)
                        if price == 0:
                            continue
                        
                        # é–“å–ã‚Š
                        layout_text = cols[3].get_text(strip=True)
                        
                        # å°‚æœ‰é¢ç©
                        area_text = cols[4].get_text(strip=True)
                        floor_area = self.parse_area(area_text)
                        if floor_area == 0:
                            continue
                        
                        # ç¯‰å¹´æ•°
                        age_text = cols[5].get_text(strip=True)
                        building_age = self.parse_building_age(age_text)
                        
                        # è©³ç´°ãƒªãƒ³ã‚¯
                        link_elem = cols[8].find('a')
                        detail_url = ''
                        if link_elem and link_elem.get('href'):
                            detail_url = urljoin('https://suumo.jp', link_elem['href'])
                        
                        # ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                        property_data = {
                            'area_id': 1,  # æ¸¯åŒº
                            'address': address,
                            'building_name': building_name,
                            'room_layout': layout_text,
                            'floor_area': floor_area,
                            'building_age': building_age,
                            'current_price': price,
                            'source_site': 'suumo',
                            'source_url': detail_url,
                            'agent_company': 'SUUMOæ²è¼‰',
                            'first_listed_at': date.today().isoformat()
                        }
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                        if self.save_property(property_data):
                            count += 1
                            self.scraped_count += 1
                            
                            if count <= 3:  # æœ€åˆã®3ä»¶ã ã‘è©³ç´°è¡¨ç¤º
                                print(f"  ğŸ“ {address} {layout_text} {floor_area}ã¡ {price:,}å††")
        
        except Exception as e:
            print(f"SUUMOç‰©ä»¶è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return count
    
    def scrape_athome(self, area_code='tokyo/minato-city'):
        """ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ï¼ˆè¦ç´„éµå®ˆç‰ˆï¼‰"""
        self.logger.info("ğŸ  ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        
        base_url = f'https://www.athome.co.jp/kodate/chuko/{area_code}/list/'
        site_url = 'https://www.athome.co.jp'
        
        # robots.txtãƒã‚§ãƒƒã‚¯
        if not self.can_fetch(site_url, f'/kodate/chuko/{area_code}/list/'):
            self.logger.warning("âš ï¸  robots.txtã«ã‚ˆã‚Šã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™")
            return 0
        
        try:
            self.logger.info("ğŸ“„ ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
            
            # é©åˆ‡ãªé…å»¶ã‚’å®Ÿè¡Œ
            self.respectful_delay('athome')
            
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’å–å¾—
            property_items = soup.find_all('div', class_='property-unit')
            
            if not property_items:
                self.logger.info("ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã§ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return 0
            
            count = 0
            max_items = min(len(property_items), 10)  # æœ€å¤§10ä»¶ã¾ã§
            
            for i, item in enumerate(property_items[:max_items]):
                if i > 0:
                    # å„ç‰©ä»¶é–“ã§ã‚‚é©åˆ‡ãªé…å»¶
                    time.sleep(random.uniform(1, 2))
                
                if self.parse_athome_item(item):
                    count += 1
                    self.scraped_count += 1
            
            self.logger.info(f"âœ… ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ å®Œäº†: {count} ä»¶ã®ç‰©ä»¶ã‚’å–å¾—")
            return count
            
        except requests.RequestException as e:
            self.logger.error(f"ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
        except Exception as e:
            self.logger.error(f"ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def parse_athome_item(self, item):
        """ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ç‰©ä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚’è§£æ"""
        try:
            # ä¾¡æ ¼
            price_elem = item.find('span', class_='price')
            if not price_elem:
                return False
            
            price_text = price_elem.get_text(strip=True)
            price = self.parse_price(price_text)
            if price == 0:
                return False
            
            # ä½æ‰€
            address_elem = item.find('div', class_='address')
            if not address_elem:
                return False
            
            address = address_elem.get_text(strip=True)
            
            # é–“å–ã‚Šãƒ»é¢ç©
            details = item.find('div', class_='property-detail')
            if not details:
                return False
            
            detail_text = details.get_text(strip=True)
            
            # é–“å–ã‚Šã‚’æŠ½å‡º
            layout_match = re.search(r'(\d+[SLDK]+)', detail_text)
            room_layout = layout_match.group(1) if layout_match else 'ä¸æ˜'
            
            # é¢ç©ã‚’æŠ½å‡º
            area_match = re.search(r'(\d+\.?\d*)ã¡', detail_text)
            floor_area = float(area_match.group(1)) if area_match else 0
            
            if floor_area == 0:
                return False
            
            # ç¯‰å¹´æ•°ã‚’æŠ½å‡º
            age_match = re.search(r'ç¯‰(\d+)å¹´', detail_text)
            building_age = int(age_match.group(1)) if age_match else None
            
            # è©³ç´°ãƒªãƒ³ã‚¯
            link_elem = item.find('a')
            detail_url = ''
            if link_elem and link_elem.get('href'):
                detail_url = urljoin('https://www.athome.co.jp', link_elem['href'])
            
            # ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            property_data = {
                'area_id': 1,  # æ¸¯åŒº
                'address': address,
                'building_name': '',
                'room_layout': room_layout,
                'floor_area': floor_area,
                'building_age': building_age,
                'current_price': price,
                'source_site': 'athome',
                'source_url': detail_url,
                'agent_company': 'ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ æ²è¼‰',
                'first_listed_at': date.today().isoformat()
            }
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            if self.save_property(property_data):
                print(f"  ğŸ“ {address} {room_layout} {floor_area}ã¡ {price:,}å††")
                return True
            
        except Exception as e:
            print(f"ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ç‰©ä»¶è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return False
    
    def parse_price(self, price_text):
        """ä¾¡æ ¼æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›"""
        try:
            # æ•°å­—ã¨å˜ä½ã‚’æŠ½å‡º
            numbers = re.findall(r'[\d,]+', price_text)
            if not numbers:
                return 0
            
            price_str = numbers[0].replace(',', '')
            price = int(price_str)
            
            # å˜ä½ã‚’ç¢ºèª
            if 'å„„' in price_text:
                price *= 100000000
            elif 'ä¸‡' in price_text:
                price *= 10000
            
            return price
            
        except (ValueError, IndexError):
            return 0
    
    def parse_area(self, area_text):
        """é¢ç©æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›"""
        try:
            match = re.search(r'(\d+\.?\d*)', area_text)
            if match:
                return float(match.group(1))
            return 0
        except ValueError:
            return 0
    
    def parse_building_age(self, age_text):
        """ç¯‰å¹´æ•°æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›"""
        try:
            match = re.search(r'(\d+)', age_text)
            if match:
                return int(match.group(1))
            return None
        except ValueError:
            return None
    
    def scrape_all(self):
        """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼ˆè¦ç´„éµå®ˆç‰ˆï¼‰"""
        self.logger.info("ğŸš€ ä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        self.logger.info("=" * 50)
        self.logger.info("âš ï¸  å„ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã¨robots.txtã‚’éµå®ˆã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™")
        self.logger.info("ğŸ“‹ å–å¾—åˆ¶é™: SUUMOæœ€å¤§5ãƒšãƒ¼ã‚¸ã€ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ æœ€å¤§10ä»¶")
        
        total_count = 0
        
        # SUUMO
        try:
            self.logger.info("1/2 SUUMOå‡¦ç†é–‹å§‹...")
            suumo_count = self.scrape_suumo()
            total_count += suumo_count
            
            # ã‚µã‚¤ãƒˆé–“ã®ååˆ†ãªé–“éš”
            inter_site_delay = random.uniform(10, 15)
            self.logger.info(f"æ¬¡ã®ã‚µã‚¤ãƒˆã¾ã§ {inter_site_delay:.1f}ç§’å¾…æ©Ÿ...")
            time.sleep(inter_site_delay)
            
        except Exception as e:
            self.logger.error(f"SUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ 
        try:
            self.logger.info("2/2 ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ å‡¦ç†é–‹å§‹...")
            athome_count = self.scrape_athome()
            total_count += athome_count
            
        except Exception as e:
            self.logger.error(f"ã‚¢ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.logger.info("=" * 50)
        self.logger.info(f"ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: åˆè¨ˆ {total_count} ä»¶ã®ç‰©ä»¶ã‚’å–å¾—")
        self.logger.info("ğŸ“Š è¦ç´„éµå®ˆã®ãŸã‚ã€å–å¾—ä»¶æ•°ã‚’åˆ¶é™ã—ã¦ã„ã¾ã™")
        
        return total_count

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    if len(sys.argv) > 1 and sys.argv[1] == '--area':
        area = sys.argv[2] if len(sys.argv) > 2 else 'minato'
        print(f"å¯¾è±¡ã‚¨ãƒªã‚¢: {area}")
    
    scraper = RealEstateScraper()
    
    try:
        total_count = scraper.scrape_all()
        print(f"\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ: {total_count} ä»¶")
        
        # çµæœç¢ºèª
        conn = scraper.get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM properties')
        total_properties = cursor.fetchone()[0]
        print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ç·ç‰©ä»¶æ•°: {total_properties} ä»¶")
        conn.close()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == '__main__':
    main()