#!/usr/bin/env python3
"""
æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³
ã‚ˆã‚Šé«˜ç²¾åº¦ãªãƒ‡ãƒ¼ã‚¿å–å¾—ã¨å‡¦ç†ã‚’å®Ÿç¾
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re
import hashlib
import random
import logging
from datetime import datetime, date
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import json
import os

class EnhancedScraper:
    def __init__(self, db_path='realestate.db'):
        self.db_path = db_path
        self.session = requests.Session()
        
        # ã‚ˆã‚Šè©³ç´°ãªUser-Agentè¨­å®š
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
        ]
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šï¼ˆã‚ˆã‚Šä¿å®ˆçš„ã«ï¼‰
        self.rate_limits = {
            'suumo': {'min_delay': 5, 'max_delay': 10, 'max_pages': 3, 'max_items': 30},
            'athome': {'min_delay': 6, 'max_delay': 12, 'max_pages': 2, 'max_items': 20},
            'homes': {'min_delay': 8, 'max_delay': 15, 'max_pages': 2, 'max_items': 20}
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.cache_dir = 'cache'
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        self.scraped_count = 0
        self.robots_cache = {}
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_db_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
        return sqlite3.connect(self.db_path)
    
    def rotate_user_agent(self):
        """User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        ua = random.choice(self.user_agents)
        self.session.headers['User-Agent'] = ua
        self.logger.debug(f"User-Agent rotated: {ua[:50]}...")
    
    def respectful_delay(self, site_name):
        """ã‚µã‚¤ãƒˆã«å¿œã˜ãŸé©åˆ‡ãªé…å»¶ã‚’å®Ÿè¡Œ"""
        if site_name in self.rate_limits:
            min_delay = self.rate_limits[site_name]['min_delay']
            max_delay = self.rate_limits[site_name]['max_delay']
            delay = random.uniform(min_delay, max_delay)
            
            # æ™‚é–“å¸¯ã«ã‚ˆã‚‹èª¿æ•´
            current_hour = datetime.now().hour
            if 9 <= current_hour <= 18:  # å–¶æ¥­æ™‚é–“å¸¯ã¯é…å»¶ã‚’å¢—åŠ 
                delay *= 1.5
            
            self.logger.info(f"{site_name}: {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(delay)
        else:
            time.sleep(random.uniform(3, 7))
    
    def get_cached_response(self, url):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—"""
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                # 1æ™‚é–“ä»¥å†…ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
                if datetime.now().timestamp() - cache_data['timestamp'] < 3600:
                    self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: {url}")
                    return cache_data['content']
        
        return None
    
    def cache_response(self, url, content):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        cache_key = hashlib.md5(url.encode()).hexdigest()
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        cache_data = {
            'url': url,
            'content': content,
            'timestamp': datetime.now().timestamp()
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False)
    
    def safe_request(self, url, retries=3):
        """å®‰å…¨ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        for attempt in range(retries):
            try:
                # User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                self.rotate_user_agent()
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
                cached_content = self.get_cached_response(url)
                if cached_content:
                    return cached_content
                
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                self.cache_response(url, response.text)
                
                return response.text
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                else:
                    self.logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ€çµ‚å¤±æ•—: {url}")
                    return None
    
    def enhanced_parse_suumo(self, html_content):
        """æ”¹è‰¯ç‰ˆSUUMOãƒšãƒ¼ã‚¸è§£æ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        properties = []
        
        # ã‚ˆã‚Šè©³ç´°ãªç‰©ä»¶æƒ…å ±ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®SUUMOã‚¯ãƒ©ã‚¹åã‚’ä½¿ç”¨ï¼‰
        property_items = soup.find_all('div', class_='property_unit-info')
        
        # ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒãƒƒã‚°ç”¨ã«æ§‹é€ ã‚’ç¢ºèª
        if not property_items:
            self.logger.warning("ç‰©ä»¶è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚HTMLã®æ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ã¾ã™...")
            # ä¾¡æ ¼ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            price_elements = soup.find_all(text=re.compile(r'ä¸‡å††|å††'))
            if price_elements:
                self.logger.info(f"ä¾¡æ ¼é–¢é€£ã®è¦ç´ ãŒ {len(price_elements)} å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                # ä¾¡æ ¼å‘¨è¾ºã®è¦ç´ ã‚’è¦ªè¦ç´ ã¨ã—ã¦å–å¾—
                for price_elem in price_elements[:3]:  # æœ€åˆã®3ã¤ã ã‘
                    parent = price_elem.parent
                    if parent:
                        property_items.append(parent.parent if parent.parent else parent)
            else:
                self.logger.warning("ä¾¡æ ¼æƒ…å ±ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        for item in property_items:
            try:
                # åŸºæœ¬æƒ…å ±
                title_elem = item.find('div', class_='cassetteitem_content-title')
                if not title_elem:
                    continue
                
                building_name = title_elem.get_text(strip=True)
                
                # ä½æ‰€æƒ…å ±
                address_elem = item.find('li', class_='cassetteitem_detail-col1')
                if not address_elem:
                    continue
                
                address = address_elem.get_text(strip=True)
                
                # äº¤é€šæƒ…å ±
                transport_elem = item.find('li', class_='cassetteitem_detail-col2')
                transport_info = transport_elem.get_text(strip=True) if transport_elem else ""
                
                # æ§‹é€ ãƒ»ç¯‰å¹´æ•°
                detail_elem = item.find('li', class_='cassetteitem_detail-col3')
                structure_age = detail_elem.get_text(strip=True) if detail_elem else ""
                
                # ç¯‰å¹´æ•°ã‚’æŠ½å‡º
                building_age = self.extract_building_age(structure_age)
                structure = self.extract_structure(structure_age)
                
                # å„éƒ¨å±‹ã®æƒ…å ±
                room_tables = item.find_all('table', class_='cassetteitem_other')
                for table in room_tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) >= 8:
                            try:
                                # éšæ•°
                                floor_info = cols[1].get_text(strip=True)
                                floor = self.extract_floor(floor_info)
                                
                                # ä¾¡æ ¼
                                price_text = cols[2].get_text(strip=True)
                                price = self.parse_price(price_text)
                                if price == 0:
                                    continue
                                
                                # é–“å–ã‚Š
                                layout_text = cols[3].get_text(strip=True)
                                
                                # å°‚æœ‰é¢ç©
                                area_text = cols[4].get_text(strip=True)
                                floor_area = self.parse_area(area_text)
                                if floor_area == 0:
                                    continue
                                
                                # ç®¡ç†è²»ãƒ»ä¿®ç¹•ç©ç«‹é‡‘
                                management_fee = self.parse_management_fee(cols[5].get_text(strip=True))
                                
                                # è©³ç´°ãƒªãƒ³ã‚¯
                                detail_url = self.extract_detail_url(cols[8])
                                
                                property_data = {
                                    'area_id': 1,  # æ¸¯åŒº
                                    'address': address,
                                    'building_name': building_name,
                                    'room_layout': layout_text,
                                    'floor_area': floor_area,
                                    'building_age': building_age,
                                    'floor': floor,
                                    'structure': structure,
                                    'current_price': price,
                                    'management_fee': management_fee,
                                    'transport_info': transport_info,
                                    'source_site': 'suumo',
                                    'source_url': detail_url,
                                    'agent_company': 'SUUMOæ²è¼‰',
                                    'first_listed_at': date.today().isoformat()
                                }
                                
                                properties.append(property_data)
                                
                            except Exception as e:
                                self.logger.warning(f"éƒ¨å±‹æƒ…å ±è§£æã‚¨ãƒ©ãƒ¼: {e}")
                                continue
                
            except Exception as e:
                self.logger.warning(f"ç‰©ä»¶æƒ…å ±è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return properties
    
    def extract_building_age(self, text):
        """ç¯‰å¹´æ•°ã‚’æŠ½å‡º"""
        match = re.search(r'ç¯‰(\d+)å¹´', text)
        if match:
            return int(match.group(1))
        return None
    
    def extract_structure(self, text):
        """æ§‹é€ ã‚’æŠ½å‡º"""
        structures = ['RC', 'SRC', 'é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ', 'é‰„éª¨é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ', 'æœ¨é€ ', 'é‰„éª¨é€ ']
        for structure in structures:
            if structure in text:
                return structure
        return None
    
    def extract_floor(self, text):
        """éšæ•°ã‚’æŠ½å‡º"""
        match = re.search(r'(\d+)éš', text)
        if match:
            return int(match.group(1))
        return None
    
    def parse_management_fee(self, text):
        """ç®¡ç†è²»ã‚’è§£æ"""
        if not text or text == '-':
            return 0
        
        match = re.search(r'(\d+,?\d*)', text.replace(',', ''))
        if match:
            return int(match.group(1).replace(',', ''))
        return 0
    
    def extract_detail_url(self, cell):
        """è©³ç´°URLã‚’æŠ½å‡º"""
        link = cell.find('a')
        if link and link.get('href'):
            return urljoin('https://suumo.jp', link['href'])
        return ''
    
    def parse_price(self, price_text):
        """ä¾¡æ ¼æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if not price_text or price_text == '-':
            return 0
        
        try:
            # ä¾¡æ ¼ç¯„å›²ã®å ´åˆã¯æœ€ä½ä¾¡æ ¼ã‚’ä½¿ç”¨
            if 'ï½' in price_text:
                price_text = price_text.split('ï½')[0]
            
            # æ•°å­—ã¨å˜ä½ã‚’æŠ½å‡º
            numbers = re.findall(r'[\d,]+', price_text)
            if not numbers:
                return 0
            
            price_str = numbers[0].replace(',', '')
            price = int(price_str)
            
            # å˜ä½ã‚’ç¢ºèª
            if 'å„„' in price_text:
                price *= 100000000
            elif 'ä¸‡' in price_text:
                price *= 10000
            elif 'thousand' in price_text.lower():
                price *= 1000
            
            return price
            
        except (ValueError, IndexError):
            return 0
    
    def parse_area(self, area_text):
        """é¢ç©æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if not area_text or area_text == '-':
            return 0
        
        try:
            # é¢ç©ç¯„å›²ã®å ´åˆã¯æœ€åˆã®å€¤ã‚’ä½¿ç”¨
            if 'ï½' in area_text:
                area_text = area_text.split('ï½')[0]
            
            match = re.search(r'(\d+\.?\d*)', area_text)
            if match:
                return float(match.group(1))
            return 0
        except ValueError:
            return 0
    
    def save_property_enhanced(self, property_data):
        """æ”¹è‰¯ç‰ˆç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # ç‰©ä»¶ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
            hash_data = f"{property_data['address']}{property_data['room_layout']}{property_data['floor_area']}{property_data.get('floor', '')}"
            property_hash = hashlib.md5(hash_data.encode()).hexdigest()
            
            # æ—¢å­˜ç‰©ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            cursor.execute('SELECT id FROM properties WHERE master_property_hash = ?', (property_hash,))
            existing = cursor.fetchone()
            
            if existing:
                property_id = existing[0]
                # ä¾¡æ ¼ã‚’æ›´æ–°
                cursor.execute('''
                    UPDATE properties 
                    SET current_price = ?, management_fee = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (property_data['current_price'], property_data.get('management_fee', 0), property_id))
                
                self.logger.info(f"  ğŸ”„ ç‰©ä»¶æ›´æ–°: {property_data['address'][:30]}...")
            else:
                # æ–°è¦ç‰©ä»¶ã‚’æŒ¿å…¥
                cursor.execute('''
                    INSERT INTO properties 
                    (area_id, address, building_name, room_layout, floor_area, building_age, 
                     floor, structure, current_price, management_fee, first_listed_at, 
                     master_property_hash, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                ''', (
                    property_data['area_id'],
                    property_data['address'],
                    property_data.get('building_name', ''),
                    property_data['room_layout'],
                    property_data['floor_area'],
                    property_data.get('building_age'),
                    property_data.get('floor'),
                    property_data.get('structure'),
                    property_data['current_price'],
                    property_data.get('management_fee', 0),
                    property_data.get('first_listed_at', date.today().isoformat()),
                    property_hash
                ))
                property_id = cursor.lastrowid
                
                self.logger.info(f"  âœ… æ–°è¦ç‰©ä»¶: {property_data['address'][:30]}...")
            
            # ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
            listing_id = hashlib.md5(f"{property_data['source_site']}{property_data['source_url']}".encode()).hexdigest()
            cursor.execute('''
                INSERT OR REPLACE INTO property_listings
                (property_id, source_site, source_url, listing_id, agent_company, 
                 listed_price, is_active, scraped_at, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            ''', (
                property_id,
                property_data['source_site'],
                property_data['source_url'],
                listing_id,
                property_data.get('agent_company', ''),
                property_data['current_price']
            ))
            
            conn.commit()
            return property_id
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            conn.rollback()
            return None
        finally:
            conn.close()
    
    def enhanced_scrape_suumo(self, area_code='13103'):
        """æ”¹è‰¯ç‰ˆSUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.logger.info("ğŸ  æ”¹è‰¯ç‰ˆSUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        
        base_url = 'https://suumo.jp/ms/chuko/tokyo/sc_minato/'
        
        count = 0
        max_pages = self.rate_limits['suumo']['max_pages']
        max_items = self.rate_limits['suumo']['max_items']
        
        for page in range(1, max_pages + 1):
            if count >= max_items:
                self.logger.info(f"æœ€å¤§å–å¾—ä»¶æ•° ({max_items}) ã«é”ã—ã¾ã—ãŸ")
                break
            
            if page == 1:
                url = base_url
            else:
                url = f"{base_url}?page={page}"
            
            try:
                self.logger.info(f"ğŸ“„ SUUMOãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å‡¦ç†ä¸­...")
                
                if page > 1:
                    self.respectful_delay('suumo')
                
                html_content = self.safe_request(url)
                if not html_content:
                    self.logger.error(f"ãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    continue
                
                properties = self.enhanced_parse_suumo(html_content)
                
                if not properties:
                    self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã«ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    break
                
                page_saved = 0
                for prop in properties:
                    if count >= max_items:
                        break
                    
                    if self.save_property_enhanced(prop):
                        page_saved += 1
                        count += 1
                        self.scraped_count += 1
                
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page} å®Œäº†: {page_saved} ä»¶ä¿å­˜")
                
            except Exception as e:
                self.logger.error(f"SUUMOãƒšãƒ¼ã‚¸ {page} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        self.logger.info(f"âœ… SUUMOå®Œäº†: {count} ä»¶ã®ç‰©ä»¶ã‚’å–å¾—")
        return count
    
    def run_enhanced_scraping(self):
        """æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        self.logger.info("=" * 50)
        
        total_count = 0
        
        # SUUMO
        try:
            self.logger.info("æ”¹è‰¯ç‰ˆSUUMOå‡¦ç†é–‹å§‹...")
            suumo_count = self.enhanced_scrape_suumo()
            total_count += suumo_count
            
            # ååˆ†ãªé–“éš”ã‚’ç©ºã‘ã‚‹
            inter_site_delay = random.uniform(15, 25)
            self.logger.info(f"æ¬¡ã®ã‚µã‚¤ãƒˆã¾ã§ {inter_site_delay:.1f}ç§’å¾…æ©Ÿ...")
            time.sleep(inter_site_delay)
            
        except Exception as e:
            self.logger.error(f"SUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.logger.info("=" * 50)
        self.logger.info(f"ğŸ‰ æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: åˆè¨ˆ {total_count} ä»¶")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        self.show_scraping_statistics()
        
        return total_count
    
    def show_scraping_statistics(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆã‚’è¡¨ç¤º"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM properties')
        total_properties = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM property_listings')
        total_listings = cursor.fetchone()[0]
        
        cursor.execute('SELECT source_site, COUNT(*) FROM property_listings GROUP BY source_site')
        site_stats = cursor.fetchall()
        
        print("\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ:")
        print(f"ç·ç‰©ä»¶æ•°: {total_properties}")
        print(f"ç·ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°æ•°: {total_listings}")
        print("ã‚µã‚¤ãƒˆåˆ¥ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°:")
        for site, count in site_stats:
            print(f"  - {site}: {count} ä»¶")
        
        conn.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = EnhancedScraper()
    
    print("ğŸš€ æ”¹è‰¯ç‰ˆä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³")
    print("=" * 40)
    
    try:
        total_count = scraper.run_enhanced_scraping()
        print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {total_count} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()